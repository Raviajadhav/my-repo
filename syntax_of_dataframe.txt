

|============================|
|1.Sales and menu Project - 1|
|============================|

Read the data from file.
------------------------
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

schema = StructType([
		 StructField("product_id", IntegerType(), True),
		 StructField("customer_id", StringType(), True),
		 StructField("order_date", DateType(), True),
		 StructField("location", StringType(), True),
		 StructField("source_order", StringType(), True)])

sales_df = spark \
		   .read \
		   .format("CSV") \
		   .option("header", True) \
		   .option("inferschema", True) \
		   .schema(schema) \
		   .option("path",'dbfs:/FileStore/shared_uploads/ravijadhav0840@gmail.com/sales_csv.txt') \
		   .load()


---------------------------------
derived month, year and quarter
---------------------------------

from pyspark.sql.functions import year, month, quarter

sales_df1 = sales_df.withColumn("order_month", month(sales_df.order_date))

sales_year = sales_df1.withColumn("order_year",year(sales_df1.order_date))

sales_quarter = sales_year.withColumn("order_quarter", quarter(sales_year.order_date))

sales_quarter.show()


-----------------------
Read the data from file
-----------------------
schemas = StructType([
    StructField("product_id",IntegerType(), True),
    StructField("product_name",StringType(), True),
    StructField("price",StringType(), True)
])

menu_df = spark \
		  .read \
		  .format("csv") \
		  .option("inferschema", True) \
		  .schema(schemas) \
		  .option("path",'dbfs:/FileStore/shared_uploads/ravijadhav0840@gmail.com/menu_csv.txt') \
		  .load()

--------------
Join Condition
--------------

join_condition = sales_df.product_id == menu_df.product_id

-----------------------------------
Total amount spent by each customer
-----------------------------------

join_df = sales_df.join(menu_df,join_condition, "inner").groupBy('customer_id').agg({'price':'sum'}).orderBy('customer_id')
display(join_df)


-----------------------------------------
Total amount spent by each food category
-----------------------------------------

sales_df.join(menu_df, join_condition, 'inner').groupBy('product_name') \
.agg({'price':'sum'}).orderBy('product_name').show()


----------------------------------------------------
Total amount of sales in each month,year and quarter
----------------------------------------------------

join_condition = sales_df1.product_id == menu_df.product_id
monthly_sales = sales_df1.join(menu_df,join_condition,'inner').groupBy(sales_df1.order_month).agg({'price':'sum'}).orderBy('order_month')
display(monthly_sales)

join_condition = sales_year.product_id == menu_df.product_id
yearly_sales = sales_year.join(menu_df, join_condition, 'inner').groupBy('order_year').agg({'price':'sum'}).orderBy('order_year')
yearly_sales.show()

join_condition = sales_quarter.product_id == menu_df.product_id
quarterly_sales = sales_quarter.join(menu_df, join_condition, 'inner').groupBy('order_quarter').agg({'price':'sum'}).orderBy('order_quarter')
quarterly_sales.show()


------------------------------------
How many times each product purchase
------------------------------------
from pyspark.sql.functions import count

join_condition = sales_df.product_id == menu_df.product_id

dataframe
category_df = (sales_df.join(menu_df, join_condition, 'inner')\
.groupBy('sales_df.product_id','product_name').agg(count('sales_df.product_id').alias('product_count'))\
.order_by('product_count'))

spark sql
order_df.createOrReplaceTempView("orders_category")
menu_df.createOrReplaceTempView("order_menu")

order_cate = spark.sql("select oc.product_id, om.product_name, count(oc.product_id)as product_count from orders_category oc join order_menu om \
on(oc.product_id=om.product_id) group by oc.product_id, product_name order by product_count desc").show()


---------------------------
Total sales by each country
---------------------------

join_condition= sales_df.product_id == menu_df.product_id

toat_sale = sales_df.join(menu_df, join_condition, 'inner').groupBy('location').agg({'price':'sum'}).alias('total_sales')

--------------------------------------------
Frequency of customer visited to restaurants
--------------------------------------------

from pyspark.sql.functions import countDistinct
freq_df = sales_df.where(sales_df.source_order == 'Restaurant')
fre_df = freq_df.groupBy('customer_id').agg(countDistinct('order_date'))
fre_df.show()


---------------------------
Total sales by order source
---------------------------
sales_order_by_source = (sales_df.join(menu_df, 'product_id').groupBy('source_order').agg({'price':'sum'}))

display(sales_order_by_source)


====================================================================================================================================
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
====================================================================================================================================

2.Play store project of pyspark - 2
===================================


To import libraries
-------------------
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import *

----------------
To read the data
----------------
play_store_df = spark.read \
.format("csv") \
.option("header",True) \
.option("inferschema", True)\
.option("path",'dbfs:/FileStore/shared_uploads/ravijadhav0840@gmail.com/googleplaystore.csv') \
.load()

-------------------
To print the schema
-------------------
play_store_df.printSchema()

-------------------------
drop the multiple columns
-------------------------
drop_df = play_store_df.drop("size","Content Rating","Last Updated","Current Ver","Android Ver")

-------------------------------
Change the datatypes of columns
-------------------------------
from pyspark.sql.functions import regexp_replace, col
	new_df = drop_df.withColumn("reviews",col("reviews").cast(IntegerType())) \
		.withColumn("installs",regexp_replace(col("installs"),"[^0-9]","")) \
			.withColumn("installs",col("installs").cast(IntegerType())) \
				.withColumn("price",regexp_replace(col("price"),"[$]","")) \
					.withColumn("price",col("price").cast(IntegerType()))

--------------
Table creation
--------------
new_df.createOrReplaceTempView("play_store")

------------------------------------
Find out top 10 reviews given to app
------------------------------------
%sql 

select App, sum(reviews) from play_store group by 1 order by 2 desc

--------------------------------------------
Category wise distribution of installed apps
--------------------------------------------
%sql 

select Category, sum(installs) from play_store group by 1 order by 2 desc

----------------------
Total paid rating apps
----------------------
%sql 

select App, sum(price) from play_store
group by 1
order by 2 desc

=================================================================================================================================
=================================================================================================================================

Some extra points:

take a list.
------------
lists = 
[
    (1,'Ravi',7000),
    (2,'kavi',8000),
    (3,'mavi',7900),
    (4,'ovi',1030),
    (5,'Rabi',7600),
    (6,'Rony',8000)
]

columns = ("emp_id", "emp_name", "salary")

Create a dataframe:1
--------------------
df = spark.createDataFrame(lists,columns)
df.show()

Create a dataframe:2
--------------------
df1 = spark.createDataFrame(lists).toDF("emp_id", "emp_name", "salary")
df1.show()


# Sorting DataFrame using sort()

df.sort("department","state").show(truncate=False)
df.sort(col("department"),col("state")).show(6,truncate=False)


-- Add New column with Null values:-
------------------------------------

	df.withColumn("New_column", lit(null))


-- Add New column with incremental values:-
-------------------------------------------
	from pyspark.sql.functions import monotonically_incresing_id

	df.withColumn("New_column", monotonically_incresing_id)




-- SQL Queries to find n th highest salary. 
===========================================

-------------------------------------------------------------------------------------
1. n th Highest salary of employee.
-----------------------------------

	with rankd_salary as 
	(
	select emp2.*, dense_rank() over(order by salary asc) as salary_number from emp2
	) 
	select rankd_salary.* 
	from rankd_salary 
	where salary_number=3;

-------------------------------------------------------------------------------------
2. Department wise Max Highest salary of employee.
--------------------------------------------------

	SELECT d.dept_name, e.emp_name, e.salary
	FROM emp2 e 
	JOIN dept d ON e.dept_id = d.dept_id 
	WHERE salary IN (SELECT MAX(salary) 
	FROM emp2 GROUP BY dept_id);

-------------------------------------------------------------------------------------
3. Department wise n th highest salary of employee.
---------------------------------------------------

	with ranked_salary as
	(
	SELECT salary,e.dept_id, e.emp_name, 
	dense_rank() over(partition by e.dept_id order by salary desc)as salary1 
	FROM emp2 e
	)
	SELECT d.dept_name, rs.emp_name, rs.salary
	FROM ranked_salary rs
	JOIN dept d ON rs.dept_id = d.dept_id
	where rs.salary1 =2;

======================================================================================
-- Some Git Commands with their description.

1.  git init: Initialize a new Git repository in the current directory.

2.  git clone [url]: Clone a remote repository into a new directory.

3.  git add [file(s)]: Add file(s) to the staging area to prepare them for a commit.

4.  git commit -m "[commit message]": Commit staged changes with a descriptive message.

5.  git status: Show the current state of the working directory and staging area.

6.  git diff: Show the differences between the working directory, staging area, and the last commit.

7.  git pull: Fetch and merge changes from the remote repository into the current branch.

8.  git push: Push local commits to the remote repository.

9.  git branch:	List all local branches in the repository.

10. git checkout [branch_name]: Switch to a different branch.

11. git merge [branch_name]: Merge changes from one branch into the current branch.

12. git remote -v: List all remote repositories configured for the current local repository.

13. git log: Show the commit history.

14. git reset [file]: Unstage a file from the staging area (use 'git reset' without a file to unstage all).

15. git stash: Stash changes in the working directory, allowing you to work on something else and then come back to it later.


